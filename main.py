import re
import sys
import time
import math
import html
import os
import requests
import pandas as pd
import xml.etree.ElementTree as ET
from urllib.parse import quote
from python_template_for_ai_assistant.title_parser import parse_ratio_title

# -----------------------------
# 1) æ¤œç´¢èªï¼ˆåŠ©è©å¿…é ˆ + å›³æ›¸ã®ã¿ã§ãƒã‚¤ã‚ºå‰Šæ¸›ï¼‰
# -----------------------------
# dpid=iss-ndl-opac ã§å›½ä¼šå›³æ›¸é¤¨è”µæ›¸ï¼ˆä¸»ã«å›³æ›¸ï¼‰ã«é™å®š
# åŠ©è©ï¼ˆã®/ãŒ/ã¯ï¼‰ã‚’å¿…é ˆã«ã—ã¦ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒç²¾åº¦UP
# NDLã‚µãƒ¼ãƒã¯å…¨è§’ãƒ»åŠè§’æ•°å­—ã‚’æ­£è¦åŒ–ã™ã‚‹ãŒã€æ¼¢æ•°å­—ã¯åˆ¥æ‰±ã„
QUERIES = [    
    # ã€ŒãŒã€ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã€Œaã¯bãŒcã€å½¢å¼ï¼‰- åŠè§’æ•°å­—
    'title="ãŒ1å‰²" AND dpid=iss-ndl-opac',
    'title="ãŒ2å‰²" AND dpid=iss-ndl-opac',
    'title="ãŒ3å‰²" AND dpid=iss-ndl-opac',
    'title="ãŒ4å‰²" AND dpid=iss-ndl-opac',
    'title="ãŒ5å‰²" AND dpid=iss-ndl-opac',
    'title="ãŒ6å‰²" AND dpid=iss-ndl-opac',
    'title="ãŒ7å‰²" AND dpid=iss-ndl-opac',
    'title="ãŒ8å‰²" AND dpid=iss-ndl-opac',
    'title="ãŒ9å‰²" AND dpid=iss-ndl-opac',
    # æ¼¢æ•°å­—
    'title="ãŒä¸€å‰²" AND dpid=iss-ndl-opac',
    'title="ãŒäºŒå‰²" AND dpid=iss-ndl-opac',
    'title="ãŒä¸‰å‰²" AND dpid=iss-ndl-opac',
    'title="ãŒå››å‰²" AND dpid=iss-ndl-opac',
    'title="ãŒäº”å‰²" AND dpid=iss-ndl-opac',
    'title="ãŒå…­å‰²" AND dpid=iss-ndl-opac',
    'title="ãŒä¸ƒå‰²" AND dpid=iss-ndl-opac',
    'title="ãŒå…«å‰²" AND dpid=iss-ndl-opac',
    'title="ãŒä¹å‰²" AND dpid=iss-ndl-opac',
]

SRU_ENDPOINT = "https://ndlsearch.ndl.go.jp/api/sru"  # å…¬å¼ä¾‹ã§ã‚‚ã“ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒæç¤ºã•ã‚Œã¦ã„ã¾ã™  [oai_citation:2â€¡å›½ç«‹å›½ä¼šå›³æ›¸é¤¨ã‚µãƒ¼ãƒï¼ˆNDLã‚µãƒ¼ãƒï¼‰](https://ndlsearch.ndl.go.jp/help/api/specifications)

# æ§ãˆã‚ã«ï¼ˆå¤§é‡ã‚¢ã‚¯ã‚»ã‚¹ã¯æ³¨æ„å–šèµ·ã‚ã‚Šï¼‰ [oai_citation:3â€¡å›½ç«‹å›½ä¼šå›³æ›¸é¤¨ã‚µãƒ¼ãƒï¼ˆNDLã‚µãƒ¼ãƒï¼‰](https://iss.ndl.go.jp/information/api/)
SLEEP_SEC = 0.25

# -----------------------------
# 2) SRUã§ã‚¿ã‚¤ãƒˆãƒ«ã‚’é›†ã‚ã‚‹
# -----------------------------
def sru_search(query: str, start_record: int = 1, maximum_records: int = 50) -> str:
    """
    SRU searchRetrieve.
    ä¾‹ã§ã¯ operation=searchRetrieve&maximumRecords=10&query=title="æ¡œ" AND from="2018" ã¨ã„ã£ãŸå½¢ã€‚ [oai_citation:4â€¡å›½ç«‹å›½ä¼šå›³æ›¸é¤¨ã‚µãƒ¼ãƒï¼ˆNDLã‚µãƒ¼ãƒï¼‰](https://ndlsearch.ndl.go.jp/help/api/specifications)
    """
    params = {
        "operation": "searchRetrieve",
        "query": query,
        "startRecord": start_record,
        "maximumRecords": maximum_records,
    }
    r = requests.get(SRU_ENDPOINT, params=params, timeout=30)
    r.raise_for_status()
    return r.text

def parse_sru(xml_text: str):
    """
    SRU XMLã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ç­‰ã‚’æŠœãï¼ˆDC-NDLãƒ™ãƒ¼ã‚¹ï¼‰ã€‚ [oai_citation:5â€¡å›½ç«‹å›½ä¼šå›³æ›¸é¤¨ã‚µãƒ¼ãƒï¼ˆNDLã‚µãƒ¼ãƒï¼‰](https://ndlsearch.ndl.go.jp/help/api/specifications)
    ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯DPã«ã‚ˆã‚Šæºã‚Œã‚‹ã®ã§ã€ã¾ãšã¯ title ã¨ identifier/link ã ã‘ã‚’å …ç‰¢ã«æ‹¾ã†ã€‚
    """
    root = ET.fromstring(xml_text)

    # åå‰ç©ºé–“ï¼ˆSRU/DCãªã©ï¼‰
    ns = {
        "srw": "http://www.loc.gov/zing/srw/",
    }

    # totalä»¶æ•°
    n = root.findtext(".//srw:numberOfRecords", default="0", namespaces=ns)
    total = int(n) if n.isdigit() else 0

    rows = []
    for rec in root.findall(".//srw:record", ns):
        # recordDataã®ä¸­èº«ã‚’å–å¾—ï¼ˆã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã•ã‚ŒãŸXMLãŒå…¥ã£ã¦ã„ã‚‹ï¼‰
        record_data = rec.findtext(".//srw:recordData", default="", namespaces=ns)
        
        if not record_data:
            continue
            
        # HTMLã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã‚’è§£é™¤
        unescaped = html.unescape(record_data)
        
        # æ­£è¦è¡¨ç¾ã§ã‚¿ã‚¤ãƒˆãƒ«ã¨identifierã‚’æŠ½å‡ºï¼ˆåå‰ç©ºé–“ã‚’è€ƒæ…®ï¼‰
        title_match = re.search(r'<dc:title>(.+?)</dc:title>', unescaped)
        title = title_match.group(1) if title_match else None
        
        # identifierã‚‚åŒæ§˜ã«æŠ½å‡º
        id_match = re.search(r'<dc:identifier>(.+?)</dc:identifier>', unescaped)
        identifier = id_match.group(1) if id_match else None

        if title:  # ã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚‹å ´åˆã®ã¿è¿½åŠ 
            # &amp; ãªã©ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚‚è§£é™¤
            title = html.unescape(title)
            rows.append({
                "source": "ndl_sru",
                "title_raw": title,
                "id_or_url": identifier,
            })
    return total, rows

def harvest_ndl(queries, per_page=50, max_pages=20, debug=False):
    all_rows = []
    for i, q in enumerate(queries, 1):
        print(f"  [{i}/{len(queries)}] {q[:30]}...", end=" ", flush=True)
        # 1ãƒšãƒ¼ã‚¸ç›®ã§ç·ä»¶æ•°ã‚’çŸ¥ã‚‹
        xml1 = sru_search(q, start_record=1, maximum_records=per_page)
        
        if debug and i == 1:
            # æœ€åˆã®ã‚¯ã‚¨ãƒªã®ç”ŸXMLã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with open("debug_response.xml", "w", encoding="utf-8") as f:
                f.write(xml1)
            print(f"\nâœ“ XMLãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ debug_response.xml ã«ä¿å­˜")
        
        total, rows = parse_sru(xml1)
        
        if debug and i == 1:
            print(f"  ãƒ‘ãƒ¼ã‚¹çµæœ: {len(rows)}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰")
            if len(rows) > 0:
                print(f"  ã‚µãƒ³ãƒ—ãƒ«: {rows[0]}")
        
        all_rows.extend(rows)
        time.sleep(SLEEP_SEC)

        pages = min(max_pages, math.ceil(total / per_page))
        for p in range(2, pages + 1):
            start = (p - 1) * per_page + 1
            xmlp = sru_search(q, start_record=start, maximum_records=per_page)
            _, rows = parse_sru(xmlp)
            all_rows.extend(rows)
            time.sleep(SLEEP_SEC)
        
        print(f"â†’ {len(rows)}ä»¶ (å…¨{total}ä»¶ä¸­)")

    df = pd.DataFrame(all_rows)
    print(f"  ç”Ÿãƒ‡ãƒ¼ã‚¿: {len(df)}ä»¶")
    
    if len(df) > 0:
        df = df.dropna(subset=["title_raw"])
        print(f"  ã‚¿ã‚¤ãƒˆãƒ«æœ‰åŠ¹: {len(df)}ä»¶")
        # ã‚¿ã‚¤ãƒˆãƒ«ã§é›‘ã«é‡è¤‡é™¤å»ï¼ˆå¾Œã§idç­‰ã§ç²¾ç·»åŒ–ã—ã¦ã‚‚OKï¼‰
        df = df.drop_duplicates(subset=["title_raw"]).reset_index(drop=True)
        print(f"  é‡è¤‡é™¤å»å¾Œ: {len(df)}ä»¶")
    
    return df

# -----------------------------
# 3) ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ a/b/c ã‚’æŠ½å‡ºï¼ˆtitle_parserã‚’ä½¿ç”¨ï¼‰
# -----------------------------

def build_rank(df: pd.DataFrame):
    if len(df) == 0:
        # ç©ºã®DataFrameã®å ´åˆã¯ç©ºã®çµæœã‚’è¿”ã™
        empty_extracted = pd.DataFrame(columns=["source", "title_raw", "id_or_url", "c_value", "c_type", "a_raw", "b_raw"])
        empty_ranking = pd.DataFrame(columns=["a_raw", "c_sum", "n", "examples"])
        return empty_extracted, empty_ranking
    
    out = df.copy()
    
    # parse_ratio_titleã§a, b, cã‚’æŠ½å‡º
    result = out["title_raw"].map(parse_ratio_title)
    out["a_raw"] = result.map(lambda x: x[0])
    out["b_raw"] = result.map(lambda x: x[1])
    out["c_value"] = result.map(lambda x: x[2])
    out["c_type"] = out["c_value"].map(lambda x: "wari" if x is not None else None)
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã™ã‚‹ã‚‚ã®ï¼ˆcãŒå–ã‚ŒãŸã‚‚ã®ï¼‰ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    matched_count = out["c_value"].notna().sum()
    print(f"  ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒ: {matched_count}ä»¶ / {len(out)}ä»¶")
    out = out[out["c_value"].notna()].reset_index(drop=True)

    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆaãŒå–ã‚Œãªã„ã‚¿ã‚¤ãƒˆãƒ«ã‚‚ã‚ã‚‹ã®ã§ã€aãŒã‚ã‚‹ã‚‚ã®ã‚’å„ªå…ˆï¼‰
    out_ab = out.dropna(subset=["a_raw"]).copy()

    if len(out_ab) == 0:
        # a_rawãŒå–ã‚ŒãŸã‚‚ã®ãŒãªã„å ´åˆ
        empty_ranking = pd.DataFrame(columns=["a_raw", "c_sum", "n", "examples"])
        return out, empty_ranking

    agg = (out_ab.groupby("a_raw")
           .agg(c_sum=("c_value","sum"),
                n=("c_value","count"))
           .sort_values(["c_sum","n"], ascending=[False, False])
           .reset_index())

    # æ¤œç®—ç”¨ã®ä»£è¡¨ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆä¸Šä½3ä»¶ï¼‰
    examples = (out_ab.groupby("a_raw")["title_raw"]
                .apply(lambda s: " / ".join(list(s.head(3))))
                .reset_index()
                .rename(columns={"title_raw":"examples"}))

    agg = agg.merge(examples, on="a_raw", how="left")
    return out, agg

def main():
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰åˆ¤å®š
    test_mode = "--test" in sys.argv
    debug_mode = "--debug" in sys.argv
    force_fetch = "--force" in sys.argv  # å¼·åˆ¶å†å–å¾—ãƒ•ãƒ©ã‚°
    
    # titles_extracted.csvãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
    if os.path.exists("titles_extracted.csv") and not force_fetch:
        print("ğŸ“„ æ—¢å­˜ã®titles_extracted.csvã‚’ä½¿ç”¨ã—ã¾ã™")
        print("   ï¼ˆå†å–å¾—ã™ã‚‹å ´åˆã¯ --force ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ï¼‰")
        extracted = pd.read_csv("titles_extracted.csv", encoding="utf-8-sig")
        print(f"âœ“ {len(extracted)}ä»¶ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    else:
        if force_fetch:
            print("ğŸ”„ --force ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚Šå†å–å¾—ã—ã¾ã™")
        
        if test_mode:
            print("ğŸ§ª ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: æœ€å°ã‚µãƒ³ãƒ—ãƒ«ã§å®Ÿè¡Œ")
            queries = QUERIES[:2]  # æœ€åˆã®2ã‚¯ã‚¨ãƒªã®ã¿
            per_page = 10
            max_pages = 1
        else:
            print("ğŸ“š æœ¬ç•ªãƒ¢ãƒ¼ãƒ‰: å…¨ã‚¯ã‚¨ãƒªã§å®Ÿè¡Œ")
            queries = QUERIES
            per_page = 50
            max_pages = 20
        
        print(f"ã‚¯ã‚¨ãƒªæ•°: {len(queries)}, ãƒšãƒ¼ã‚¸/ã‚¯ã‚¨ãƒª: {max_pages}, ä»¶æ•°/ãƒšãƒ¼ã‚¸: {per_page}")
        print("å–å¾—é–‹å§‹...")
        
        df_titles = harvest_ndl(queries, per_page=per_page, max_pages=max_pages, debug=debug_mode or test_mode)
        print(f"âœ“ {len(df_titles)}ä»¶ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—")
        
        extracted, ranking = build_rank(df_titles)
        extracted.to_csv("titles_extracted.csv", index=False, encoding="utf-8-sig")
        print("âœ“ titles_extracted.csvã‚’ä¿å­˜ã—ã¾ã—ãŸ")
    
    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã å ´åˆã‚‚ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’å†è¨ˆç®—
    if os.path.exists("titles_extracted.csv") and not force_fetch:
        # extractedã‹ã‚‰ç›´æ¥ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’ä½œæˆã™ã‚‹ãŸã‚ã€å…ƒã®DataFrameã‚’å†æ§‹ç¯‰
        df_for_ranking = extracted[["source", "title_raw", "id_or_url"]].copy() if "source" in extracted.columns else pd.DataFrame({"source": "ndl_sru", "title_raw": extracted["title_raw"], "id_or_url": extracted.get("id_or_url", None)})
        _, ranking = build_rank(df_for_ranking)

    ranking.to_csv("a_ranking.csv", index=False, encoding="utf-8-sig")

    print("\nSaved:")
    print(" - a_ranking.csv")
    
    if len(ranking) > 0:
        print(f"\nTop 20 (å…¨{len(ranking)}ä»¶):")
        print(ranking.head(20).to_string(index=False))
    else:
        print("\nâš ï¸  ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãªã—")
    
    if test_mode:
        print("\nğŸ’¡ æœ¬ç•ªå®Ÿè¡Œã¯: python main.py")

if __name__ == "__main__":
    main()